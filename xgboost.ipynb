{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fits the baseline model. It predicts on the test data without doing any feature engineering, using the xgboost library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv').drop(['id'], axis=1)\n",
    "df_train['source'] = 'simulation'\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "test_ids = df_test.id\n",
    "dt_test = df_test.drop(['id'], axis=1)\n",
    "\n",
    "df_supp = pd.read_csv('data/cirrhosis.csv').drop(['ID'], axis=1)\n",
    "df_supp['source'] = 'original'\n",
    "\n",
    "# merge supplemental data\n",
    "df_train = pd.concat([df_train, df_supp]).reset_index(drop=True)\n",
    "train_target = df_train['Status']\n",
    "\n",
    "TARGET = 'Status'\n",
    "SKEWED_FEATS = ['Bilirubin', 'Cholesterol', 'Copper', 'Prothrombin', 'Alk_Phos']\n",
    "CAT_FEATS = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "NUM_FEATS = [x for x in df_train.columns if x not in CAT_FEATS and x != TARGET]\n",
    "NUM_FEATS.remove('source')\n",
    "ORIG_FEATS = df_train.drop(TARGET, axis=1).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data for XGBoost\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(data, skewed_features):\n",
    "    df_log_feats = pd.DataFrame()\n",
    "    for col in skewed_features:\n",
    "        name = f'{col}_log'\n",
    "        df_log_feats[name] = np.log(data[col])\n",
    "    return df_log_feats\n",
    "\n",
    "def encode(data):\n",
    "    X_raw = data.dropna().copy()\n",
    "    X_raw = X_raw.iloc[:, 0:18]\n",
    "\n",
    "    # encode the categorical features\n",
    "    X_cat = X_raw[CAT_FEATS]\n",
    "    X_num = np.array(X_raw[NUM_FEATS])\n",
    "    oe = OrdinalEncoder()\n",
    "    X_cat = oe.fit_transform(X_cat)\n",
    "    X_cat = pd.DataFrame(X_cat, columns=CAT_FEATS)\n",
    "    X_num = pd.DataFrame(X_num, columns=NUM_FEATS)\n",
    "    return X_cat, X_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and target arrays\n",
    "X, y = df_train.drop(TARGET, axis=1), df_train[[TARGET]]\n",
    "\n",
    "# encode features\n",
    "X_cat, X_num = encode(X)\n",
    "X = pd.concat([X_cat, X_num], axis=1)\n",
    "\n",
    "# encode target\n",
    "le = LabelEncoder()\n",
    "y_mi = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n symptoms\n",
    "# recode edema \"S\" and \"Y\" to both indicate edema for the purpose of this count variable\n",
    "X['Edema'] = np.where(X['Edema'] == 2, 1, X['Edema'])\n",
    "X['N_Symptoms'] = X.Edema + X.Spiders + X.Ascites + X.Hepatomegaly\n",
    "\n",
    "# logarithmic transformations\n",
    "def get_logs(data, skewed_features):\n",
    "    X_log = X[skewed_features]\n",
    "    other_feats = data.columns.difference(skewed_features).tolist()\n",
    "    X_other = X[other_feats]\n",
    "    X_log = log_transformation(X_log, skewed_features)\n",
    "    return pd.concat([X_log, X_other], axis=1)\n",
    "\n",
    "X = get_logs(X, SKEWED_FEATS)\n",
    "\n",
    "# Age at Diagnosis\n",
    "X['Dgns_Age'] = X['Age'] - X['N_Days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do train/test split on the data\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create regression matrices in XGBoost's data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "d_test = xgb.DMatrix(X_dev, y_dev, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model\n",
    "\n",
    "## Define wandb sweep parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\", # sweep method\n",
    "    \"metric\": {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"booster\": { # tree-based or linear functions?\n",
    "            \"values\": [\"gbtree\", \"gblinear\"]\n",
    "        },\n",
    "        \"max_depth\": { # depth of individual trees\n",
    "            \"values\": [3, 6, 9, 12]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.05, 0.2]\n",
    "        },\n",
    "        \"subsample\": { # the proportion of instances to take a sub-sample per iteration\n",
    "            \"values\": [1, 0.5, 0.3] \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "def train(data_train, data_dev, model_note, objective='multi:softprob', n_rounds=10_000, early_stopping_rounds=100, lr=.1):\n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        # objective for multiclass classification.\n",
    "        # for regression it would be reg:squarederror, for example.\n",
    "        \"objective\": objective,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_class\": 3,\n",
    "        \"tree_method\": \"hist\"} # optimization without a GPU\n",
    "\n",
    "    model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=data_train,\n",
    "    num_boost_round=n_rounds,\n",
    "    evals=data,\n",
    "    early_stopping_rounds=early_stopping_rounds\n",
    "    )\n",
    "\n",
    "    # get predictions on dev set\n",
    "    y_pred = model.predict()\n",
    "\n",
    "    model.save_model(f'checkpoints/{model_note}_{time.time()}.json') \n",
    "    # with a categorical target, if its not json then it throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_note = \"initial_fe\"\n",
    "# specify the names of the data for xgboost to use\n",
    "evals = [(dtrain, \"train\"), (dtest, \"validation\")]\n",
    "# baseline\n",
    "train(data=evals, model_note=model_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "train(model_note=\"first\", early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to-do\n",
    "\n",
    "1. cross-validation\n",
    "2. hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
