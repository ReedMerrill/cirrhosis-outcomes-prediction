{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fits the baseline model. It predicts on the test data without doing any feature engineering, using the xgboost library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8323, 20)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv').drop(['id'], axis=1)\n",
    "df_train['source'] = 'simulation'\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "test_ids = df_test.id\n",
    "dt_test = df_test.drop(['id'], axis=1)\n",
    "\n",
    "df_supp = pd.read_csv('data/cirrhosis.csv').drop(['ID'], axis=1)\n",
    "df_supp['source'] = 'original'\n",
    "\n",
    "# merge supplemental data\n",
    "df_train = pd.concat([df_train, df_supp]).reset_index(drop=True)\n",
    "train_target = df_train['Status']\n",
    "\n",
    "TARGET = 'Status'\n",
    "SKEWED_FEATS = ['Bilirubin', 'Cholesterol', 'Copper', 'Prothrombin', 'Alk_Phos']\n",
    "CAT_FEATS = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "NUM_FEATS = [x for x in df_train.columns if x not in CAT_FEATS and x != TARGET]\n",
    "NUM_FEATS.remove('source')\n",
    "ORIG_FEATS = df_train.drop(TARGET, axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data\n",
    "\n",
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(data, skewed_features):\n",
    "    df_log_feats = pd.DataFrame()\n",
    "    for col in skewed_features:\n",
    "        name = f'{col}_log'\n",
    "        df_log_feats[name] = np.log(data[col])\n",
    "    return df_log_feats\n",
    "\n",
    "def encode(data):\n",
    "    X_raw = data.iloc[:, 0:18]\n",
    "\n",
    "    # encode the categorical features\n",
    "    X_cat = X_raw[CAT_FEATS]\n",
    "    X_num = np.array(X_raw[NUM_FEATS])\n",
    "    oe = OrdinalEncoder()\n",
    "    X_cat = oe.fit_transform(X_cat)\n",
    "    X_cat = pd.DataFrame(X_cat, columns=CAT_FEATS)\n",
    "    X_num = pd.DataFrame(X_num, columns=NUM_FEATS)\n",
    "    return X_cat, X_num\n",
    "    \n",
    "# logarithmic transformations\n",
    "def get_logs(data, skewed_features):\n",
    "    X_log = X[skewed_features]\n",
    "    other_feats = data.columns.difference(skewed_features).tolist()\n",
    "    X_other = X[other_feats]\n",
    "    X_log = log_transformation(X_log, skewed_features)\n",
    "    return pd.concat([X_log, X_other], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8323, 18)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create features and target arrays\n",
    "X, y = df_train.drop(TARGET, axis=1), df_train[[TARGET]]\n",
    "\n",
    "# encode features\n",
    "X_cat, X_num = encode(X)\n",
    "X = pd.concat([X_cat, X_num], axis=1)\n",
    "\n",
    "# encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(np.ravel(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8323, 20)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recode edema \"S\" and \"Y\" to both indicate edema for the purpose of this count variable\n",
    "X['Edema'] = np.where(X['Edema'] == 2, 1, X['Edema'])\n",
    "# Number of diseases/symptoms related to liver disease\n",
    "X['N_Symptoms'] = X.Edema + X.Spiders + X.Ascites + X.Hepatomegaly\n",
    "# log transformations\n",
    "X = get_logs(X, SKEWED_FEATS)\n",
    "# Age at Diagnosis\n",
    "X['Dgns_Age'] = X['Age'] - X['N_Days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6658, 20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do train/test split on the data\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit XGBoost Model\n",
    "\n",
    "## Define wandb sweep parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sweep_config = {\n",
    "    \"method\": \"random\", # sweep method\n",
    "    \"metric\": {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"booster\": { # tree-based or linear functions?\n",
    "            \"value\": 'gbtree'\n",
    "        },\n",
    "        \"max_depth\": { # depth of individual trees\n",
    "            \"values\": [6, 7]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            'distribution': 'uniform',\n",
    "            'min': .04,\n",
    "            'max': .09,\n",
    "        },\n",
    "        \"subsample\": { # the proportion of instances to take as a sub-sample per iteration\n",
    "            'distribution': 'uniform',\n",
    "            'min': .33,\n",
    "            'max': 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "xgb_sweep_id = wandb.sweep(xgb_sweep_config, project='cirrhosis-xgb-sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "def train():\n",
    "    config_defaults = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"max_depth\": 3,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 1,\n",
    "        \"seed\": 1,\n",
    "    }\n",
    "\n",
    "    wandb.init(config=config_defaults) # set defaults. They'll be over-ridden later.\n",
    "    config = wandb.config\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=10_000,\n",
    "        early_stopping_rounds=1,\n",
    "        learning_rate=config.learning_rate,\n",
    "        booster=config.booster,\n",
    "        max_depth=config.max_depth,\n",
    "        subsample=config.subsample\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_dev, y_dev)])\n",
    "\n",
    "    # get predictions on dev set\n",
    "    y_pred = xgb_model.predict(X_dev)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_dev, predictions)\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "\n",
    "    #model.save_model(f'checkpoints/{config.booster}\"-\"{config.max_depth}\"-\"{config.learning_rate}\"-\"{config.subsample}\"-\"{time.time()}.json') \n",
    "    # with a categorical target, if its not json then it throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the sweep, the training function, and the number of sweeps\n",
    "wandb.agent(xgb_sweep_id, train, count=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to-do\n",
    "\n",
    "- [] switch wandb metric from accuracy to val loss. Seems like the config's metric and the one logged using wandb.log() need to be the same.\n",
    "\n",
    "# Deep Neural Network Model\n",
    "\n",
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# one-hot encode the y vector\n",
    "y_train_oh = pd.get_dummies(y_train)\n",
    "y_dev_oh = pd.get_dummies(y_dev)\n",
    "\n",
    "# model.fit() requires tf.float32 data types. It's easiest to make the conversion before the data is a tf.dataset\n",
    "y_train_oh = np.float32(y_train_oh)\n",
    "y_dev_oh = np.float32(y_dev_oh)\n",
    "\n",
    "# get input shape\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# convert to tf.dataset\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train_oh))\n",
    "dev = tf.data.Dataset.from_tensor_slices((X_dev, y_dev_oh))\n",
    "\n",
    "# specify the batch size\n",
    "train = train.batch(1)\n",
    "dev = dev.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=n_features))\n",
    "    model.add(layers.Dense(4, activation='relu'))\n",
    "    model.add(layers.Dense(\n",
    "        units=3,\n",
    "        activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(.001),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "class WandbMetricsLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        wandb.log(logs)\n",
    "    \n",
    "# set callbacks\n",
    "callbacks = [\n",
    "    WandbMetricsLogger()\n",
    "    ]\n",
    "\n",
    "# train the model\n",
    "model.fit(\n",
    "    train,\n",
    "    epochs=1,\n",
    "    validation_data=dev)\n",
    "    #callbacks=callbacks) # uncomment for wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided not to do hyperparameter tuning on this model because it isn't funtioning very well. There are a few reasons why this might be:\n",
    "\n",
    "1. Not enough data. This could be getting exascerbated by my choice to do a training/validation split rather k-fold cross validation.\n",
    "2. The features, as they stand, aren't appropriate for logistic regression. the Sci-kit Learn logistic regression does L1/L2 regularization by default, probably because often when we work with tabular data the features are messy.\n",
    "3. My feature engineering didn't account for extreme values well enough. \n",
    "4. There are highly correlated features (like N_Days and Dgns_Age and Age) that ruin the optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
