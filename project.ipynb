{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv').drop(['id'], axis=1)\n",
    "df_train['source'] = 'simulation'\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "test_ids = df_test.id\n",
    "dt_test = df_test.drop(['id'], axis=1)\n",
    "\n",
    "df_supp = pd.read_csv('data/cirrhosis.csv').drop(['ID'], axis=1)\n",
    "df_supp['source'] = 'original'\n",
    "\n",
    "# merge supplemental data\n",
    "df_train = pd.concat([df_train, df_supp]).reset_index(drop=True)\n",
    "train_target = df_train['Status']\n",
    "\n",
    "TARGET = 'Status'\n",
    "SKEWED_FEATS = ['Bilirubin', 'Cholesterol', 'Copper', 'Prothrombin', 'Alk_Phos']\n",
    "CAT_FEATS = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "NUM_FEATS = [x for x in df_train.columns if x not in CAT_FEATS and x != TARGET]\n",
    "NUM_FEATS.remove('source')\n",
    "ORIG_FEATS = df_train.drop(TARGET, axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data\n",
    "\n",
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(data, skewed_features):\n",
    "    df_log_feats = pd.DataFrame()\n",
    "    for col in skewed_features:\n",
    "        name = f'{col}_log'\n",
    "        df_log_feats[name] = np.log(data[col])\n",
    "    return df_log_feats\n",
    "\n",
    "def encode(data):\n",
    "    X_raw = data.iloc[:, 0:18]\n",
    "\n",
    "    # encode the categorical features\n",
    "    X_cat = X_raw[CAT_FEATS]\n",
    "    X_num = np.array(X_raw[NUM_FEATS])\n",
    "    oe = OrdinalEncoder()\n",
    "    X_cat = oe.fit_transform(X_cat)\n",
    "    X_cat = pd.DataFrame(X_cat, columns=CAT_FEATS)\n",
    "    X_num = pd.DataFrame(X_num, columns=NUM_FEATS)\n",
    "    return X_cat, X_num\n",
    "    \n",
    "# logarithmic transformations\n",
    "def get_logs(data, skewed_features):\n",
    "    X_log = X[skewed_features]\n",
    "    other_feats = data.columns.difference(skewed_features).tolist()\n",
    "    X_other = X[other_feats]\n",
    "    X_log = log_transformation(X_log, skewed_features)\n",
    "    return pd.concat([X_log, X_other], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and target arrays\n",
    "X, y = df_train.drop(TARGET, axis=1), df_train[[TARGET]]\n",
    "\n",
    "# encode features\n",
    "X_cat, X_num = encode(X)\n",
    "X = pd.concat([X_cat, X_num], axis=1)\n",
    "\n",
    "# encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(np.ravel(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode edema \"S\" and \"Y\" to both indicate edema for the purpose of this count variable\n",
    "X['Edema'] = np.where(X['Edema'] == 2, 1, X['Edema'])\n",
    "# Number of diseases/symptoms related to liver disease\n",
    "X['N_Symptoms'] = X.Edema + X.Spiders + X.Ascites + X.Hepatomegaly\n",
    "# log transformations\n",
    "X = get_logs(X, SKEWED_FEATS)\n",
    "# Age at Diagnosis\n",
    "X['Dgns_Age'] = X['Age'] - X['N_Days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       Bilirubin_log  Cholesterol_log  Copper_log  Prothrombin_log  \\\n",
      "2812      -0.510826         5.697093    4.060443         2.272126   \n",
      "7638      -0.693147         5.513429    3.663562         2.312535   \n",
      "8121       2.433613         6.410175    4.174387         2.379546   \n",
      "8069       1.386294         5.278115    4.382027         2.424803   \n",
      "4278       1.193922         5.170484    5.634790         2.433613   \n",
      "...             ...              ...         ...              ...   \n",
      "2895       1.987874         7.151485    5.509388         2.379546   \n",
      "7813      -0.510826         5.407172    3.784190         2.397895   \n",
      "905        1.609438         7.377759    4.317488         2.388763   \n",
      "5192       1.163151         5.616771    4.736198         2.302585   \n",
      "235       -0.356675         5.463832    4.077537         2.282382   \n",
      "\n",
      "      Alk_Phos_log      Age  Albumin  Ascites  Drug  Edema  Hepatomegaly  \\\n",
      "2812      6.712956  12641.0     3.50      0.0   1.0    0.0           1.0   \n",
      "7638      7.068172  17774.0     3.61      0.0   1.0    0.0           0.0   \n",
      "8121      7.489971  13178.0     3.31      0.0   1.0    0.0           1.0   \n",
      "8069      7.822445  19470.0     3.45      0.0   0.0    0.0           1.0   \n",
      "4278      8.575462  17874.0     3.83      0.0   0.0    0.0           0.0   \n",
      "...            ...      ...      ...      ...   ...    ...           ...   \n",
      "2895      7.793174  11058.0     3.77      0.0   1.0    0.0           1.0   \n",
      "7813      6.326149  16279.0     3.54      0.0   0.0    0.0           1.0   \n",
      "905       7.884577  19270.0     3.40      0.0   0.0    0.0           1.0   \n",
      "5192      7.068172  16374.0     3.18      0.0   1.0    0.0           0.0   \n",
      "235       7.246368  11058.0     3.66      0.0   0.0    0.0           1.0   \n",
      "\n",
      "      N_Days  N_Symptoms  Platelets    SGOT  Sex  Spiders  Stage  \\\n",
      "2812   930.0         1.0      336.0   65.10  1.0      0.0    3.0   \n",
      "7638  2692.0         1.0      165.0   75.00  0.0      1.0    2.0   \n",
      "8121   790.0         1.0      298.0  151.90  0.0      0.0    3.0   \n",
      "8069  1077.0         1.0      212.0  133.30  1.0      0.0    3.0   \n",
      "4278  4191.0         1.0      295.0  128.65  0.0      1.0    1.0   \n",
      "...      ...         ...        ...     ...  ...      ...    ...   \n",
      "2895  1234.0         1.0      330.0  198.40  0.0      0.0    1.0   \n",
      "7813  3707.0         2.0      280.0   43.40  0.0      1.0    3.0   \n",
      "905   1657.0         2.0      181.0   82.15  0.0      1.0    3.0   \n",
      "5192   890.0         1.0      200.0  110.05  0.0      1.0    1.0   \n",
      "235   2657.0         2.0      493.0   89.90  0.0      1.0    2.0   \n",
      "\n",
      "      Tryglicerides  Dgns_Age  \n",
      "2812           55.0   11711.0  \n",
      "7638          106.0   15082.0  \n",
      "8121          210.0   12388.0  \n",
      "8069          142.0   18393.0  \n",
      "4278          177.0   13683.0  \n",
      "...             ...       ...  \n",
      "2895          140.0    9824.0  \n",
      "7813          124.0   12572.0  \n",
      "905           174.0   17613.0  \n",
      "5192           91.0   15484.0  \n",
      "235            86.0    8401.0  \n",
      "\n",
      "[6658 rows x 20 columns]>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8943/3921187781.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do train/test split on the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "# do train/test split on the data\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "print(X_train.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_cols = X_train.columns\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_cols)\n",
    "y_train_imputed = pd.DataFrame(imputer.transform(X_dev), columns=X_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit XGBoost Model\n",
    "\n",
    "## Define wandb sweep parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sweep_config = {\n",
    "    \"method\": \"random\", # sweep method\n",
    "    \"metric\": {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"booster\": { # tree-based or linear functions?\n",
    "            \"value\": 'gbtree'\n",
    "        },\n",
    "        \"max_depth\": { # depth of individual trees\n",
    "            \"values\": [6, 7]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            'distribution': 'uniform',\n",
    "            'min': .04,\n",
    "            'max': .09,\n",
    "        },\n",
    "        \"subsample\": { # the proportion of instances to take as a sub-sample per iteration\n",
    "            'distribution': 'uniform',\n",
    "            'min': .33,\n",
    "            'max': 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "xgb_sweep_id = wandb.sweep(xgb_sweep_config, project='cirrhosis-xgb-sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "def train():\n",
    "    config_defaults = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"max_depth\": 3,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 1,\n",
    "        \"seed\": 1,\n",
    "    }\n",
    "\n",
    "    wandb.init(config=config_defaults) # set defaults. They'll be over-ridden later.\n",
    "    config = wandb.config\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=10_000,\n",
    "        early_stopping_rounds=1,\n",
    "        learning_rate=config.learning_rate,\n",
    "        booster=config.booster,\n",
    "        max_depth=config.max_depth,\n",
    "        subsample=config.subsample\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_dev, y_dev)])\n",
    "\n",
    "    # get predictions on dev set\n",
    "    y_pred = xgb_model.predict(X_dev)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_dev, predictions)\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "\n",
    "    #model.save_model(f'checkpoints/{config.booster}\"-\"{config.max_depth}\"-\"{config.learning_rate}\"-\"{config.subsample}\"-\"{time.time()}.json') \n",
    "    # with a categorical target, if its not json then it throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the sweep, the training function, and the number of sweeps\n",
    "wandb.agent(xgb_sweep_id, train, count=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to-do\n",
    "\n",
    "- [] switch wandb metric from accuracy to val loss. Seems like the config's metric and the one logged using wandb.log() need to be the same.\n",
    "\n",
    "# Deep Neural Network Model\n",
    "\n",
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# one-hot encode the y vector\n",
    "y_train_oh = pd.get_dummies(y_train)\n",
    "y_dev_oh = pd.get_dummies(y_dev)\n",
    "\n",
    "# model.fit() requires tf.float32 data types. It's easiest to make the conversion before the data is a tf.dataset\n",
    "y_train_oh = np.float32(y_train_oh)\n",
    "y_dev_oh = np.float32(y_dev_oh)\n",
    "\n",
    "# get input shape\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# convert to tf.dataset\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train_oh))\n",
    "dev = tf.data.Dataset.from_tensor_slices((X_dev, y_dev_oh))\n",
    "\n",
    "# specify the batch size\n",
    "train = train.batch(1)\n",
    "dev = dev.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=n_features))\n",
    "    model.add(layers.Dense(4, activation='relu'))\n",
    "    model.add(layers.Dense(\n",
    "        units=3,\n",
    "        activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(.001),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "class WandbMetricsLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        wandb.log(logs)\n",
    "    \n",
    "# set callbacks\n",
    "callbacks = [\n",
    "    WandbMetricsLogger()\n",
    "    ]\n",
    "\n",
    "# train the model\n",
    "#model.fit(\n",
    "#   train,\n",
    "#   epochs=1,\n",
    "#   validation_data=dev,\n",
    "#   callbacks=callbacks) # uncomment for wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided not to do hyperparameter tuning on this model because it isn't funtioning very well. There are a few reasons why this might be:\n",
    "\n",
    "1. Not enough data. This could be getting exascerbated by my choice to do a training/validation split rather k-fold cross validation.\n",
    "2. The features, as they stand, aren't appropriate for logistic regression. Compared to my Keras implementation, Sci-kit Learn's logistic regression does L1/L2 regularization by default, probably because often when we work with tabular data the features are messy.\n",
    "3. My feature engineering didn't account for extreme values well enough. \n",
    "4. There are highly correlated features (like N_Days and Dgns_Age and Age) that might be ruining the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn Logistic Regression\n",
    "\n",
    "## Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4166\n",
      "2    2265\n",
      "1     227\n",
      "Name: count, dtype: int64\n",
      "{0: 1.5981757081132981, 2: 2.939514348785872, 1: 29.330396475770925}\n"
     ]
    }
   ],
   "source": [
    "class_counts = pd.Series(y_train).value_counts()\n",
    "n_train = X_train.shape[0]\n",
    "class_weights = [n_train / count for count in class_counts]\n",
    "class_weights = dict(zip([0, 2, 1], class_weights))\n",
    "print(class_counts)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight={0: 1.5981757081132981, 1: 29.330396475770925,\n",
       "                                 2: 2.939514348785872},\n",
       "                   max_iter=500, multi_class=&#x27;ovr&#x27;, solver=&#x27;newton-cholesky&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight={0: 1.5981757081132981, 1: 29.330396475770925,\n",
       "                                 2: 2.939514348785872},\n",
       "                   max_iter=500, multi_class=&#x27;ovr&#x27;, solver=&#x27;newton-cholesky&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight={0: 1.5981757081132981, 1: 29.330396475770925,\n",
       "                                 2: 2.939514348785872},\n",
       "                   max_iter=500, multi_class='ovr', solver='newton-cholesky')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    multi_class='ovr',\n",
    "    solver='newton-cholesky',\n",
    "    class_weight=class_weights,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "logit.fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Cholesky solver is good for when the number of instances is much greater than the number of features, especially if there are one-hot encoded features. This explains why the model converged with this solver but not others. \n",
    "\n",
    "Failed solvers: \"liblinear\", \"saga\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
