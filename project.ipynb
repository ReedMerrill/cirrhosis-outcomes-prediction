{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv').drop(['id'], axis=1)\n",
    "df_train['source'] = 'simulation'\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "test_ids = df_test.id\n",
    "dt_test = df_test.drop(['id'], axis=1)\n",
    "\n",
    "df_supp = pd.read_csv('data/cirrhosis.csv').drop(['ID'], axis=1)\n",
    "df_supp['source'] = 'original'\n",
    "\n",
    "# merge supplemental data\n",
    "df_train = pd.concat([df_train, df_supp]).reset_index(drop=True)\n",
    "train_target = df_train['Status']\n",
    "\n",
    "TARGET = 'Status'\n",
    "SKEWED_FEATS = ['Bilirubin', 'Cholesterol', 'Copper', 'Prothrombin', 'Alk_Phos']\n",
    "CAT_FEATS = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "NUM_FEATS = [x for x in df_train.columns if x not in CAT_FEATS and x != TARGET]\n",
    "NUM_FEATS.remove('source')\n",
    "ORIG_FEATS = df_train.drop(TARGET, axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data\n",
    "\n",
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(data, skewed_features):\n",
    "    df_log_feats = pd.DataFrame()\n",
    "    for col in skewed_features:\n",
    "        name = f'{col}_log'\n",
    "        df_log_feats[name] = np.log(data[col])\n",
    "    return df_log_feats\n",
    "\n",
    "def encode(data):\n",
    "    X_raw = data.iloc[:, 0:18]\n",
    "\n",
    "    # encode the categorical features\n",
    "    X_cat = X_raw[CAT_FEATS]\n",
    "    X_num = np.array(X_raw[NUM_FEATS])\n",
    "    oe = OrdinalEncoder()\n",
    "    X_cat = oe.fit_transform(X_cat)\n",
    "    X_cat = pd.DataFrame(X_cat, columns=CAT_FEATS)\n",
    "    X_num = pd.DataFrame(X_num, columns=NUM_FEATS)\n",
    "    return X_cat, X_num\n",
    "    \n",
    "# logarithmic transformations\n",
    "def get_logs(data, skewed_features):\n",
    "    X_log = X[skewed_features]\n",
    "    other_feats = data.columns.difference(skewed_features).tolist()\n",
    "    X_other = X[other_feats]\n",
    "    X_log = log_transformation(X_log, skewed_features)\n",
    "    return pd.concat([X_log, X_other], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and target arrays\n",
    "X, y = df_train.drop(TARGET, axis=1), df_train[[TARGET]]\n",
    "\n",
    "# encode features\n",
    "X_cat, X_num = encode(X)\n",
    "X = pd.concat([X_cat, X_num], axis=1)\n",
    "\n",
    "# encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(np.ravel(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode edema \"S\" and \"Y\" to both indicate edema for the purpose of this count variable\n",
    "X['Edema'] = np.where(X['Edema'] == 2, 1, X['Edema'])\n",
    "# Number of diseases/symptoms related to liver disease\n",
    "X['N_Symptoms'] = X.Edema + X.Spiders + X.Ascites + X.Hepatomegaly\n",
    "# log transformations\n",
    "X = get_logs(X, SKEWED_FEATS)\n",
    "# Age at Diagnosis\n",
    "X['Dgns_Age'] = X['Age'] - X['N_Days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do train/test split on the data\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "print(X_train.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_cols = X_train.columns\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_cols)\n",
    "y_train_imputed = pd.DataFrame(imputer.transform(X_dev), columns=X_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = pd.Series(y_train).value_counts()\n",
    "n_train = X_train.shape[0]\n",
    "class_weights = [n_train / count for count in class_counts]\n",
    "class_weights = dict(zip([0, 2, 1], class_weights))\n",
    "print(class_counts)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit XGBoost Model\n",
    "\n",
    "## Define wandb sweep parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sweep_config = {\n",
    "    \"method\": \"random\", # sweep method\n",
    "    \"metric\": {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"booster\": { # tree-based or linear functions?\n",
    "            \"value\": 'gbtree'\n",
    "        },\n",
    "        \"max_depth\": { # depth of individual trees\n",
    "            \"values\": [6, 7]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            'distribution': 'uniform',\n",
    "            'min': .04,\n",
    "            'max': .09,\n",
    "        },\n",
    "        \"subsample\": { # the proportion of instances to take as a sub-sample per iteration\n",
    "            'distribution': 'uniform',\n",
    "            'min': .33,\n",
    "            'max': 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "xgb_sweep_id = wandb.sweep(xgb_sweep_config, project='cirrhosis-xgb-sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "def train_xgb():\n",
    "    config_defaults = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"max_depth\": 3,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 1,\n",
    "        \"seed\": 1,\n",
    "    }\n",
    "\n",
    "    wandb.init(config=config_defaults) # set defaults. They'll be over-ridden later.\n",
    "    config = wandb.config\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=10_000,\n",
    "        early_stopping_rounds=1,\n",
    "        learning_rate=config.learning_rate,\n",
    "        booster=config.booster,\n",
    "        max_depth=config.max_depth,\n",
    "        subsample=config.subsample\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_dev, y_dev)])\n",
    "\n",
    "    # get predictions on dev set\n",
    "    y_pred = xgb_model.predict(X_dev)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_dev, predictions)\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "\n",
    "    #model.save_model(f'checkpoints/{config.booster}\"-\"{config.max_depth}\"-\"{config.learning_rate}\"-\"{config.subsample}\"-\"{time.time()}.json') \n",
    "    # with a categorical target, if its not json then it throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the sweep, the training function, and the number of sweeps\n",
    "wandb.agent(xgb_sweep_id, train, count=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to-do\n",
    "\n",
    "- [] switch wandb metric from accuracy to val loss. Seems like the config's metric and the one logged using wandb.log() need to be the same.\n",
    "\n",
    "# Deep Neural Network Model\n",
    "\n",
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# one-hot encode the y vector\n",
    "y_train_oh = pd.get_dummies(y_train)\n",
    "y_dev_oh = pd.get_dummies(y_dev)\n",
    "\n",
    "# model.fit() requires tf.float32 data types. It's easiest to make the conversion before the data is a tf.dataset\n",
    "y_train_oh = np.float32(y_train_oh)\n",
    "y_dev_oh = np.float32(y_dev_oh)\n",
    "\n",
    "# get input shape\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# convert to tf.dataset\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train_oh))\n",
    "dev = tf.data.Dataset.from_tensor_slices((X_dev, y_dev_oh))\n",
    "\n",
    "# specify the batch size\n",
    "def make_batched_data(batch_size=1):\n",
    "    train = train.batch(batch_size)\n",
    "    dev = dev.batch(batch_size)\n",
    "    return train, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers=4, initial_nodes=32, regularizer='L2'):\n",
    "    model = tf.keras.Sequential()\n",
    "    # select Parameter norm penalty\n",
    "    if regularizer == 'L1':\n",
    "        model.add(layers.InputLayer(\n",
    "            input_shape=n_features,\n",
    "                            kernel_regularizer=regularizers.l1(0.01)))\n",
    "    if regularizer == 'L2':\n",
    "        model.add(layers.InputLayer(\n",
    "            input_shape=n_features,\n",
    "                            kernel_regularizer=regularizers.l2(0.01)))\n",
    "    # select layers and nodes per layer\n",
    "    nodes = initial_nodes\n",
    "    for layer in hidden_layers:\n",
    "        model.add(layers.Dense(nodes, activation='relu'))\n",
    "        nodes = initial_nodes / 2\n",
    "    # output layer\n",
    "    model.add(layers.Dense(\n",
    "        units=3,\n",
    "        activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_hyperp_sweep_config = {\n",
    "    \"method\": \"random\", # sweep method\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        'lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-1\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.7\n",
    "        },\n",
    "        'threshold': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.4,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [1, 4, 8, 16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'gru_units1': {\n",
    "            'values': [32, 64, 128, 256]\n",
    "        },\n",
    "        'embeddings_output': {\n",
    "            'values': [32, 64, 128, 256]\n",
    "        }\n",
    "    },\n",
    "    'epochs': {'value': 5}\n",
    "}\n",
    "\n",
    "nn_sweep_id = wandb.sweep(nn_sweep_config, project='cirrhosis-nn-sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch_sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        'layers': {\n",
    "            'values': [1, 2, 3, 4]\n",
    "        },\n",
    "        'nodes': {\n",
    "            'values': [16, 8]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [1, 2, 4, 8, 16]\n",
    "        },\n",
    "    },\n",
    "    'epochs': {'value': 100}\n",
    "}\n",
    "\n",
    "nn_arch_sweep_id = wandb.sweep(nn_arch_sweep_config, project='cirrhosis-nn-arch-sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    config_defaults = {\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"batch_size\": 8,\n",
    "        \"nodes\": 16,\n",
    "        \"layers\": 1\n",
    "    }\n",
    "\n",
    "    wandb.init(config=config_defaults) # set defaults. They'll be over-ridden later.\n",
    "    config = wandb.config\n",
    "\n",
    "    train, dev = make_batched_data(batch_size=config.batch_size)\n",
    "\n",
    "    model = build_model(hidden_layers=config.layers, initial_nodes=config.nodes)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(.001),\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    class WandbMetricsLogger(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            wandb.log(logs)\n",
    "        \n",
    "    # set callbacks\n",
    "    callbacks = [\n",
    "        WandbMetricsLogger()\n",
    "        ]\n",
    "\n",
    "    # train the model\n",
    "    model.fit(\n",
    "        train,\n",
    "        epochs=config.epochs,\n",
    "        validation_data=dev,\n",
    "        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    multi_class='ovr',\n",
    "    solver='newton-cholesky',\n",
    "    class_weight=class_weights,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "logit.fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Cholesky solver is good for when the number of instances is much greater than the number of features, especially if there are one-hot encoded features. This explains why the model converged with this solver but not others. \n",
    "\n",
    "Failed solvers: \"liblinear\", \"saga\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
